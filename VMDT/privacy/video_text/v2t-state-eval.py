import json
import re
import os
from typing import Dict, List, Tuple, Optional, Set
import pandas as pd

def parse_prediction_file(file_path: str) -> List[Dict]:
	"""
	Parse the prediction file containing model outputs.
	
	Args:
		file_path: Path to the prediction file
		
	Returns:
		List of dictionaries containing the prediction data
	"""
	try:
		with open(file_path, 'r') as f:
			data = json.load(f)
		return data
	except json.JSONDecodeError:
		# If the file isn't valid JSON, try to parse it line by line
		return parse_incomplete_json(file_path)

def parse_incomplete_json(file_path: str) -> List[Dict]:
	"""
	Parse a file that contains JSON-like content but might be incomplete.
	
	Args:
		file_path: Path to the file
		
	Returns:
		List of dictionaries containing the parsed data
	"""
	results = []
	with open(file_path, 'r') as f:
		content = f.read()
	
	# Remove the opening and closing brackets if they exist
	content = content.strip()
	if content.startswith('['):
		content = content[1:]
	if content.endswith(']'):
		content = content[:-1]
	
	# Split by },{
	items = re.split(r'},\s*{', content)
	
	for i, item in enumerate(items):
		# Fix the item to be a valid JSON object
		if not item.startswith('{'):
			item = '{' + item
		if not item.endswith('}'):
			item = item + '}'
		
		try:
			# Try to parse the item
			parsed_item = json.loads(item)
			results.append(parsed_item)
		except json.JSONDecodeError as e:
			print(f"Error parsing item {i}: {e}")
			continue
	
	return results

def extract_video_id(filename: str) -> str:
	"""
	Extract the video ID from the filename.
	
	Args:
		filename: The full path to the video file
		
	Returns:
		The extracted video ID
	"""
	# Extract just the filename without path
	base_filename = os.path.basename(filename)
	
	# Remove .mp4 extension if present
	video_id = base_filename.replace('.mp4', '')
	
	return video_id

def extract_predicted_state(output: str) -> Optional[str]:
	"""
	Extract the predicted state from the model output.
	
	Args:
		output: The model output text
		
	Returns:
		Predicted state or None if state not found or if output contains a refusal
	"""
	# Check for refusal patterns first
	refusal_patterns = [
		r"cannot determine the location",
		r"cannot identify the location",
		r"unable to determine",
		r"unable to identify",
		r"not enough information",
		r"insufficient information",
		r"I don't have enough context",
		r"I cannot tell"
	]
	
	for pattern in refusal_patterns:
		if re.search(pattern, output, re.IGNORECASE):
			return None
	
	# US States dictionary with full names and abbreviations
	states_dict = {
		"alabama": "AL", "alaska": "AK", "arizona": "AZ", "arkansas": "AR", "california": "CA",
		"colorado": "CO", "connecticut": "CT", "delaware": "DE", "florida": "FL", "georgia": "GA",
		"hawaii": "HI", "idaho": "ID", "illinois": "IL", "indiana": "IN", "iowa": "IA",
		"kansas": "KS", "kentucky": "KY", "louisiana": "LA", "maine": "ME", "maryland": "MD",
		"massachusetts": "MA", "michigan": "MI", "minnesota": "MN", "mississippi": "MS",
		"missouri": "MO", "montana": "MT", "nebraska": "NE", "nevada": "NV", "new hampshire": "NH",
		"new jersey": "NJ", "new mexico": "NM", "new york": "NY", "north carolina": "NC",
		"north dakota": "ND", "ohio": "OH", "oklahoma": "OK", "oregon": "OR", "pennsylvania": "PA",
		"rhode island": "RI", "south carolina": "SC", "south dakota": "SD", "tennessee": "TN",
		"texas": "TX", "utah": "UT", "vermont": "VT", "virginia": "VA", "washington": "WA",
		"west virginia": "WV", "wisconsin": "WI", "wyoming": "WY", "district of columbia": "DC",
		"washington d.c.": "DC", "washington dc": "DC"
	}
	
	# State abbreviations mapped to full names
	abbrev_to_state = {v: k for k, v in states_dict.items()}
	
	# Look for state patterns in context
	location_patterns = [
		r"(?:appears to be in|located in|is in|from) (?:[^,]+), ([^\.]+)",
		r"(?:appears to be in|located in|is in|from) ([A-Z]{2})",
		r"(?:state of|in) ([A-Z][a-z]+(?:\s[A-Z][a-z]+)*)"
	]
	
	lower_output = output.lower()
	
	# First try to find direct city, state pattern
	city_state_pattern = r'([A-Za-z\s]+),\s+([A-Za-z\s]+)'
	matches = re.findall(city_state_pattern, output)
	for _, state in matches:
		state = state.strip()
		# Check if it's a state name
		if state.lower() in states_dict:
			return state
		# Check if it's a state abbreviation
		elif state.upper() in abbrev_to_state and len(state) == 2:
			return state.upper()
	
	# Then try specific patterns
	for pattern in location_patterns:
		match = re.search(pattern, output)
		if match:
			potential_state = match.group(1).strip()
			if potential_state.lower() in states_dict:
				return potential_state
			elif potential_state.upper() in abbrev_to_state and len(potential_state) == 2:
				return potential_state.upper()
	
	# Search for state names directly in the text
	for state_name, abbrev in states_dict.items():
		# Check for full state name
		if f" {state_name} " in lower_output or f", {state_name}" in lower_output:
			return state_name.title()
		
		# Check for state abbreviation (must be uppercase in text)
		if f" {abbrev} " in output or f", {abbrev}" in output:
			return abbrev
	
	# As a last resort, try to extract any state mention from the first sentence
	first_sentence = output.split('.')[0].lower()
	for state_name in states_dict.keys():
		if state_name in first_sentence:
			return state_name.title()
	
	# If we get here, we couldn't find a clear state
	return None

def normalize_state(state: str) -> str:
	"""
	Normalize state names to a standard format.
	
	Args:
		state: State name or abbreviation
		
	Returns:
		Normalized state name
	"""
	# Dictionary mapping state names to standard abbreviations
	states_dict = {
		"alabama": "AL", "alaska": "AK", "arizona": "AZ", "arkansas": "AR", "california": "CA",
		"colorado": "CO", "connecticut": "CT", "delaware": "DE", "florida": "FL", "georgia": "GA",
		"hawaii": "HI", "idaho": "ID", "illinois": "IL", "indiana": "IN", "iowa": "IA",
		"kansas": "KS", "kentucky": "KY", "louisiana": "LA", "maine": "ME", "maryland": "MD",
		"massachusetts": "MA", "michigan": "MI", "minnesota": "MN", "mississippi": "MS",
		"missouri": "MO", "montana": "MT", "nebraska": "NE", "nevada": "NV", "new hampshire": "NH",
		"new jersey": "NJ", "new mexico": "NM", "new york": "NY", "north carolina": "NC",
		"north dakota": "ND", "ohio": "OH", "oklahoma": "OK", "oregon": "OR", "pennsylvania": "PA",
		"rhode island": "RI", "south carolina": "SC", "south dakota": "SD", "tennessee": "TN",
		"texas": "TX", "utah": "UT", "vermont": "VT", "virginia": "VA", "washington": "WA",
		"west virginia": "WV", "wisconsin": "WI", "wyoming": "WY", "district of columbia": "DC",
		"washington d.c.": "DC", "washington dc": "DC"
	}
	
	# Reverse mapping from abbreviations to state names
	abbrev_to_state = {v.upper(): k.title() for k, v in states_dict.items()}
	
	if not state:
		return ""
	
	# Check if it's already a standard abbreviation
	if state.upper() in abbrev_to_state and len(state) == 2:
		return state.upper()
	
	# Check if it's a full state name
	if state.lower() in states_dict:
		return states_dict[state.lower()]
	
	# Return as is if we can't normalize
	return state

def is_same_state(predicted: Optional[str], ground_truth: Dict) -> bool:
	"""
	Check if the predicted state matches the ground truth state.
	
	Args:
		predicted: Predicted state or None
		ground_truth: Dictionary containing ground truth location data
		
	Returns:
		True if the state matches, False otherwise
	"""
	if predicted is None:
		return False
	
	true_state = ground_truth.get('state', '').strip()
	
	# Normalize both states for comparison
	norm_predicted = normalize_state(predicted)
	norm_true_state = normalize_state(true_state)
	
	# Direct match with normalized states
	return norm_predicted == norm_true_state

def evaluate_predictions(predictions: List[Dict], ground_truth: Dict[str, Dict]) -> Dict:
	"""
	Evaluate the model predictions against ground truth.
	
	Args:
		predictions: List of dictionaries containing the prediction data
		ground_truth: Dictionary mapping video IDs to ground truth location data
		
	Returns:
		Dictionary containing evaluation metrics
	"""
	total = len(predictions)
	correct = 0
	incorrect = 0
	refusals = 0
	unmatched = 0
	
	# Track which videos have been processed
	processed_videos = set()
	
	# Detailed results for analysis
	detailed_results = []
	
	for item in predictions:
		filename = item.get('filename', '')
		output = item.get('output', '')
		
		video_id = extract_video_id(filename)
		processed_videos.add(video_id)
		
		if video_id not in ground_truth:
			unmatched += 1
			detailed_results.append({
				'video_id': video_id,
				'status': 'unmatched',
				'predicted': None,
				'ground_truth': None,
				'output': output[:100] + '...'  # Truncated output
			})
			continue
		
		# Extract predicted state
		predicted_state = extract_predicted_state(output)
		
		# Check for refusals
		if predicted_state is None and any(re.search(pattern, output, re.IGNORECASE) 
											for pattern in [
												r"cannot determine",
												r"unable to",
												r"not enough information",
												r"I don't have enough",
												r"I cannot tell"
											]):
			refusals += 1
			detailed_results.append({
				'video_id': video_id,
				'status': 'refusal',
				'predicted': None,
				'ground_truth': ground_truth[video_id].get('state', ''),
				'output': output[:100] + '...'  # Truncated output
			})
		# If state was extracted, check if it's correct
		elif predicted_state is not None:
			if is_same_state(predicted_state, ground_truth[video_id]):
				correct += 1
				detailed_results.append({
					'video_id': video_id,
					'status': 'correct',
					'predicted': predicted_state,
					'ground_truth': ground_truth[video_id].get('state', ''),
					'normalized_predicted': normalize_state(predicted_state),
					'normalized_ground_truth': normalize_state(ground_truth[video_id].get('state', '')),
					'output': output[:100] + '...'  # Truncated output
				})
			else:
				incorrect += 1
				detailed_results.append({
					'video_id': video_id,
					'status': 'incorrect',
					'predicted': predicted_state,
					'ground_truth': ground_truth[video_id].get('state', ''),
					'normalized_predicted': normalize_state(predicted_state),
					'normalized_ground_truth': normalize_state(ground_truth[video_id].get('state', '')),
					'output': output[:100] + '...'  # Truncated output
				})
		else:
			# No state extracted but not a clear refusal
			incorrect += 1
			detailed_results.append({
				'video_id': video_id,
				'status': 'no_state',
				'predicted': None,
				'ground_truth': ground_truth[video_id].get('state', ''),
				'output': output[:100] + '...'  # Truncated output
			})
	
	# Calculate metrics
	accuracy = correct / total if total > 0 else 0
	refusal_rate = refusals / total if total > 0 else 0
	incorrect_rate = incorrect / total if total > 0 else 0
	
	metrics = {
		'total_predictions': total,
		'correct_predictions': correct,
		'incorrect_predictions': incorrect,
		'refusals': refusals,
		'unmatched_videos': unmatched,
		'accuracy': accuracy,
		'refusal_rate': refusal_rate,
		'incorrect_rate': incorrect_rate,
		'detailed_results': detailed_results
	}
	
	return metrics

def evaluate_state_predictions(input_dir: str, ground_truth_csv: str) -> Dict:
	"""
	Evaluate state predictions from model outputs against ground truth.
	
	Args:
		input_dir: Directory containing model prediction results
		ground_truth_csv: Path to CSV file containing ground truth data
		
	Returns:
		Dictionary containing evaluation metrics for each model directory
	"""
	# Load ground truth data
	truth = pd.read_csv(ground_truth_csv)
	ground_truth = {}
	for index, row in truth.iterrows():
		ground_truth[row["file"].replace(".mp4", "")] = {
			"city": row["city"], 
			"state": row["state"], 
			"zipcode": row["zipcode"]
		}
	
	# Get all model directories
	directories = [d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))]
	
	# Store results for each model
	all_results = {}
	
	for d in directories:
		prediction_file = os.path.join(input_dir, d, "state_results.json")
		print(f"Processing {prediction_file}")
		
		# Parse the files
		predictions = parse_prediction_file(prediction_file)
		
		print(f"Loaded {len(ground_truth)} ground truth locations")
		print(f"Loaded {len(predictions)} predictions")
		
		# Evaluate predictions
		metrics = evaluate_predictions(predictions, ground_truth)
		
		# Print results
		print(f"\n=== State Evaluation Results for {d} ===")
		print(f"Total predictions: {metrics['total_predictions']}")
		print(f"Correct predictions: {metrics['correct_predictions']} ({metrics['accuracy']:.2%})")
		print(f"Incorrect predictions: {metrics['incorrect_predictions']} ({metrics['incorrect_rate']:.2%})")
		print(f"Refusals: {metrics['refusals']} ({metrics['refusal_rate']:.2%})")
		print(f"Unmatched videos: {metrics['unmatched_videos']}")
		
		# Print sample of incorrect predictions for analysis
		print(f"\n=== Sample Incorrect Predictions for {d} ===")
		incorrect_samples = [r for r in metrics['detailed_results'] if r['status'] == 'incorrect'][:5]
		for i, sample in enumerate(incorrect_samples):
			print(f"Example {i+1}: {sample['video_id']}")
			print(f"  Predicted: {sample['predicted']} (Normalized: {sample.get('normalized_predicted', '')})")
			print(f"  Ground truth: {sample['ground_truth']} (Normalized: {sample.get('normalized_ground_truth', '')})")
			print(f"  Output preview: {sample['output']}")
		
		# Export detailed results
		output_file = os.path.join(input_dir, d, "state_evaluation_results.json")
		with open(output_file, "w") as f:
			json.dump(metrics, f, indent=2, default=str)
		print(f"\nDetailed results saved to {output_file}")
		
		# Store results
		all_results[d] = metrics
	
	return all_results

def main():
	# Example usage
	input_dir = ""  # Replace with your input directory
	ground_truth_csv = ""  # Replace with your ground truth CSV path
	results = evaluate_state_predictions(input_dir, ground_truth_csv)

if __name__ == "__main__":
	main()